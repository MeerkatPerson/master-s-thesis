from typing import Dict, Tuple
# use the 'OrderedDict' type to ensure that data emission events within a stream are sorted by key, where key = time
from collections import OrderedDict

from globals import MICROSECONDS_PER_SECOND


def parse_tgen_traffic(dir: str, num: int = None) -> Dict:

    client_file = open(f"../../tgen-traces/{dir}/client{num}.tgen.stdout")
    client_lines = client_file.readlines()

    server_file = open(f"../../tgen-traces/{dir}/server{num}.tgen.stdout")
    server_lines = server_file.readlines()

    data_dict = {'streams': dict()}

    # start with the client stdout file; it contains the stream creation information as well as information on data emission in the direction of server

    # grab the start time (in seconds)
    start_time = int(client_lines[0].split(" ")[2].split(".")[0])

    for client_line in client_lines:

        client_line_chunks = client_line.split(" ")

        if client_line_chunks[5] == "[_tgengenerator_createStream]":
            # 1st kind of line we are interested in: stream creation, i.e. lines looking like this:
            """
            2000-01-01 00:05:53 946685153.000000 [message] [tgen-generator.c:448] [_tgengenerator_createStream] [T] STREAM with mmodel seed 2743373598 was successfully generated by flow with mmodel seed 3591772533
            """

            # parse the seeds of the stream's mmodel
            stream_mm_seed = int(client_line_chunks[11])

            # check for collisions (highly unlikely that there will be any)
            if stream_mm_seed in data_dict['streams']:
                print("Stream mmodel seed collision!")

            # parse creation time, adjusted for the start_time we determined above
            # split time_unix_seconds.time_microseconds
            time = client_line_chunks[2].split(".")

            # grab unix seconds
            time_unix_seconds = int(time[0])

            # adjust for start time
            time_adjusted_seconds = time_unix_seconds - start_time

            # grab microseconds
            time_microseconds = int(time[1])

            # compute total microseconds
            time_total = time_adjusted_seconds * MICROSECONDS_PER_SECOND + time_microseconds

            data_dict['streams'].update(
                {stream_mm_seed: {'time_created': time_total, 'emissions': dict()}})

        elif client_line_chunks[5] == "[_tgenstream_flushOut]":

            # 2nd kind of line we are interested in: data emission, i.e. lines looking like this:
            """
            2000-01-01 00:05:53 946685153.574350 [message] [tgen-stream.c:1026] [_tgenstream_flushOut] [T] Stream with mmodel seed 2743373598 wrote 261 bytes to network
            """

            # parse the seed of the parent stream's mmodel
            stream_mm_seed = int(client_line_chunks[11])

            # parse creation time, adjusted for the start_time we determined above
            # split time_unix_seconds.time_microseconds
            time = client_line_chunks[2].split(".")

            # grab unix seconds
            time_unix_seconds = int(time[0])

            # adjust for start time
            time_adjusted_seconds = time_unix_seconds - start_time

            # grab microseconds
            time_microseconds = int(time[1])

            # compute total microseconds
            time_total = time_adjusted_seconds * MICROSECONDS_PER_SECOND + time_microseconds

            # parse the number of bytes emitted
            num_bytes = int(client_line_chunks[13])

            if time_total in data_dict['streams'][stream_mm_seed]['emissions']:

                # print("Time collision in data emission client stdout file, appending!")

                data_dict['streams'][stream_mm_seed]['emissions'][time_total].append(
                    {'direction': 'TO_SERVER', 'bytes': num_bytes})

            else:

                data_dict['streams'][stream_mm_seed]['emissions'].update(
                    {time_total: [{'direction': 'TO_SERVER', 'bytes': num_bytes}]})

        else:

            pass

    # now we're adding the data emission for the other direction, which we will parse from the server stdout

    for server_line in server_lines:

        server_line_chunks = server_line.split(" ")

        if server_line_chunks[5] == "[_tgenstream_flushOut]":

            # (only) kind of line we are interested in: data emission, i.e. lines looking like this:
            """
            2000-01-01 00:05:53 946685153.574350 [message] [tgen-stream.c:1026] [_tgenstream_flushOut] [T] Stream with mmodel seed 2743373598 wrote 261 bytes to network
            """

            # parse the seed of the parent stream's mmodel (luckily, the stream mmodel seeds are the same on the client vs server side)
            stream_mm_seed = int(server_line_chunks[11])

            # parse creation time, adjusted for the start_time we determined above
            # split time_unix_seconds.time_microseconds
            time = server_line_chunks[2].split(".")

            # grab unix seconds
            time_unix_seconds = int(time[0])

            # adjust for start time
            time_adjusted_seconds = time_unix_seconds - start_time

            # grab microseconds
            time_microseconds = int(time[1])

            # compute total microseconds
            time_total = time_adjusted_seconds * MICROSECONDS_PER_SECOND + time_microseconds

            # parse the number of bytes emitted
            num_bytes = int(server_line_chunks[13])

            if time_total in data_dict['streams'][stream_mm_seed]['emissions']:

                # print("Time collision in data emission server stdout file, appending!")

                data_dict['streams'][stream_mm_seed]['emissions'][time_total].append(
                    {'direction': 'TO_ORIGIN', 'bytes': num_bytes})

            else:

                data_dict['streams'][stream_mm_seed]['emissions'].update(
                    {time_total: [{'direction': 'TO_ORIGIN', 'bytes': num_bytes}]})

        else:

            pass

    # additionally, we want to ensure that
    # - emission events within a stream are sorted (why would they not be sorted in the first place? - Because we are parsing the server stdout after the client stdout, and a server emission may lie in between two existing client emissions temporally)
    # - we have delays in between creation of streams, as well as delays in between emission events on a stream available
    stream_data = data_dict['streams']

    for stream_key, stream_val in stream_data.items():

        # store the stream creation time so we can compute the delay until the first emission on this flow

        # note that since we computed all creation times in relation to the start time of the simulation (by subtracting it), the creation time of the first stream also corresponds to the delay in between starting the simulation and creating this stream.

        # initialize 'last_creation_event' with the parent stream's creation time, since for the first emission created by this stream, we want to know the delay in between the parent stream's creation and the creation of this emissionâˆ‚
        last_creation_event = stream_val['time_created']

        sorted_emission_events = OrderedDict(
            sorted(stream_val['emissions'].items()))

        data_dict['streams'][stream_key]['emissions'] = dict()

        # bootstrap last emission event
        last_emission_event = last_creation_event

        # now let's also compute the delays for the emission events on this stream
        for emission_key, emission_val in sorted_emission_events.items():

            # emission_key = emission_time; now compute the delay in between this emission and the previous one respectively the creation of the stream if this is the first emission event
            delay = emission_key - last_emission_event

            # sanity check
            if delay < 0:
                print("Something went wrong, negative delay!! (Emission level)")

            # now we're making a keeeeewwwwl dict key which is a tuple of the form (time, delay) where time is the respective timestamp in microseconds adjusted for start time and the delay is the inter-emission delay so we know how long to wait in between emissions.
            # this ensures uniqueness of dict keys, if we'd use the delays as dict keys we'd likely overwrite data from time to time due to collisions
            key: Tuple[int, int] = (emission_key, delay)

            data_dict['streams'][stream_key]['emissions'].update({
                key: emission_val})

            last_emission_event = emission_key

    # lastly, we need the delays in between stream creation (just a few values)
    # note that since we computed all creation times in relation to the start time of the simulation (by subtracting it), the creation time of the first stream also corresponds to the delay in between starting the simulation and creating this stream.
    stream_creation_time = 0

    for stream_key, stream_val in stream_data.items():

        delay = stream_val['time_created'] - stream_creation_time

        # sanity check
        if delay < 0:
            print("Something went wrong, negative delay!! (Stream level)")

        stream_val.update(
            {'delay': delay})

        stream_creation_time = stream_val['time_created']

    return data_dict
